# -*- coding: utf-8 -*-
"""Untitled10.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oVHliIIRPCBiVByutiU1HIc7yNMnXHQT

# Trabalho de modelagem e estatística

# Introdução

O trabalho a seguir será realizado com base em um banco de dados sintético, criado para simular padrões globais reais de diferentes tipos de desastres. A Variável alvo do banco é severity_level para regressão, e is_major_disaster para classificação, e serão feitas modelagens para descobrir as relações entre elas e outras variáveis para definir possíveis predições. Uma possível hipótese de negócio para a utilização do nosso trabalho seria na formação de uma rede de inteligência em agências de resgate, para definir quais desastres precisam de maior envio de pessoal, e quais precisam de maior apoio financeiro, por exemplo.

Fonte do dataset: https://www.kaggle.com/datasets/emirhanakku/disaster-events-2025?resource=download

## Descrição de colunas

Column - Description

event_id	- Unique event ID

disaster_type	- Type of natural disaster

location	- Country of the event

latitude / longitude	- Geolocation of the event

date	- Event date

severity_level	- Intensity score (1–10)

affected_population	- Estimated number of affected people

estimated_economic_loss_usd	- Financial loss estimation

response_time_hours	- First response time

aid_provided - Relief effort provided (Yes/No)

infrastructure_damage_index	- Damage score (0–1)

is_major_disaster	- Target label for ML models
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import statsmodels.api as sm
import sklearn as sk
import pycaret as pc

df = pd.read_csv('synthetic_disaster_events_2025.csv')
df.head()

"""#EDA e preparação

A seguir, serão realizadas as seguinte tarefas:

- Inspeção inicial de dados

- Observação e tratemento de valores ausentes

- Visualização da distribuição de outliers

- Visualização de relacionamentos em Pairplot

- Heatmap de correlação

- Análise final da preparação

## Inspeção inicial de dados
"""

df.info()
print("\n" + "-"*30 + "\n") # Separator for better readability
df.describe()

"""##Observação e tratemento de valores ausentes"""

missing_values = df.isnull().sum()
print("Missing values per column:")
print(missing_values)

"""##Visualização da distribuição de outliers"""

import matplotlib.pyplot as plt
import seaborn as sns

# Identify numerical columns as specified in the instructions
numerical_cols = [
    'event_id',
    'latitude',
    'longitude',
    'severity_level',
    'affected_population',
    'estimated_economic_loss_usd',
    'response_time_hours',
    'infrastructure_damage_index',
    'is_major_disaster'
]

# Generate histograms and boxplots for each numerical column
for col in numerical_cols:
    fig, axes = plt.subplots(1, 2, figsize=(15, 6))

    # Histogram
    sns.histplot(df[col], kde=True, ax=axes[0])
    axes[0].set_title(f'Distribution of {col}')
    axes[0].set_xlabel(col)
    axes[0].set_ylabel('Frequency')

    # Boxplot
    sns.boxplot(y=df[col], ax=axes[1])
    axes[1].set_title(f'Boxplot of {col}')
    axes[1].set_ylabel(col)

    plt.tight_layout()
    plt.show()

"""##Visualização de relacionamentos em Pairplot"""

import seaborn as sns
import matplotlib.pyplot as plt

# Select relevant numerical columns for the pairplot, including the target variables
pairplot_cols = [
    'severity_level',
    'affected_population',
    'estimated_economic_loss_usd',
    'response_time_hours',
    'infrastructure_damage_index',
    'is_major_disaster',
    'latitude',
    'longitude'
]

# Create the pairplot
sns.pairplot(df[pairplot_cols])
plt.suptitle('Pairwise Relationships of Selected Numerical Features', y=1.02)
plt.show()



"""##Heatmap de correlação"""

numerical_cols_for_corr = df.select_dtypes(include=np.number).columns
correlation_matrix = df[numerical_cols_for_corr].corr()

plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)
plt.title('Correlation Matrix of Numerical Features')
plt.show()

"""## Análise dos Achados da EDA

#### 1. Inspeção Inicial dos Dados
*   O DataFrame `df` contém 20.000 entradas e 13 colunas.
*   As colunas `event_id`, `severity_level`, `affected_population`, `is_major_disaster` são do tipo `int64`.
*   As colunas `latitude`, `longitude`, `estimated_economic_loss_usd`, `response_time_hours`, `infrastructure_damage_index` são do tipo `float64`.
*   As colunas `disaster_type`, `location`, `date`, `aid_provided` são do tipo `object`.
*   A coluna `date` está como `object` e deve ser convertida para `datetime` para análises temporais, se necessário.
*   Estatísticas descritivas para colunas numéricas: `severity_level` varia de 1 a 10 com média de 5.49, `affected_population` e `estimated_economic_loss_usd` mostram grande variabilidade e valores máximos significativamente mais altos que a média, indicando a presença de outliers e/ou distribuições assimétricas.

#### 2. Tratamento de Valores Ausentes
*   Não foram encontrados valores ausentes em nenhuma das colunas (`df.isnull().sum()` retornou zero para todas as colunas).
*   Portanto, nenhuma ação de imputação ou remoção foi necessária.

#### 3. Distribuições e Outliers (Histogramas e Boxplots)
*   **severity_level**: Distribuição relativamente uniforme, com picos em 3, 5, 8 e 10. Não há outliers aparentes.
*   **affected_population**: Distribuição assimétrica à direita, com uma cauda longa. Boxplot revela a presença de muitos outliers de valores altos, indicando que uma minoria de eventos afeta um número muito maior de pessoas.
*   **estimated_economic_loss_usd**: Similar à `affected_population`, também apresenta assimetria à direita e muitos outliers de valores altos. Isso sugere que a maioria dos desastres tem perdas econômicas menores, mas alguns causam perdas financeiras muito significativas.
*   **response_time_hours**: Distribuição relativamente uniforme, variando de 1 a 71.99 horas. O boxplot não mostra outliers extremos.
*   **infrastructure_damage_index**: Concentra-se mais na faixa média (0.4 a 0.7), mas varia de 0.06 a 1.00. Não há outliers extremos, mas a distribuição é um pouco assimétrica.
*   **latitude/longitude**: Distribuições multimodais, refletindo a concentração de eventos em diferentes regiões geográficas. Não há outliers extremos na perspectiva de boxplots, mas sim agrupamentos.
*   **event_id**: Distribuído uniformemente, como esperado para um ID único.
*   **is_major_disaster**: Variável binária (0 ou 1), sem outliers.

#### 4. Relações (Pairplot)
*   **severity_level** e **affected_population**: Há uma correlação positiva clara. Níveis de severidade mais altos tendem a estar associados a populações mais afetadas. No entanto, a dispersão é considerável, indicando que outros fatores também influenciam.
*   **severity_level** e **estimated_economic_loss_usd**: Também apresentam uma correlação positiva, com perdas econômicas maiores para desastres de maior severidade, embora com dispersão.
*   **affected_population** e **estimated_economic_loss_usd**: Fortemente correlacionados, o que é esperado, pois desastres que afetam mais pessoas geralmente causam mais danos econômicos.
*   **is_major_disaster** e outras variáveis: A variável `is_major_disaster` (binária) mostra alguma distinção visual em relação a `severity_level`, `affected_population` e `estimated_economic_loss_usd`, com desastres maiores tendendo a ter valores mais altos nessas métricas. Isso é um bom indicativo para modelos de classificação.
*   Outras combinações: A maioria das outras relações visuais no pairplot mostra pouca correlação linear direta ou padrões complexos que podem exigir modelos não-lineares.

#### 5. Mapa de Correlação
*   **Correlações Fortes Positivas**:
    *   `affected_population` e `severity_level` (0.88): Correlação muito forte, indicando que desastres mais severos afetam mais pessoas.
    *   `estimated_economic_loss_usd` e `severity_level` (0.83): Correlação forte, desastres mais severos resultam em maiores perdas econômicas.
    *   `estimated_economic_loss_usd` e `affected_population` (0.97): Correlação extremamente forte e esperada, o impacto financeiro está diretamente ligado ao número de pessoas afetadas.
*   **Correlações Moderadas a Fracas**:
    *   `is_major_disaster` tem correlações moderadas com `severity_level` (0.40), `affected_population` (0.41) e `estimated_economic_loss_usd` (0.41), confirmando que esses são bons preditores para identificar desastres maiores.
    *   `infrastructure_damage_index` e `severity_level` (0.33): Correlação positiva moderada, maior severidade está ligada a maior índice de dano.
    *   `response_time_hours` mostra correlações muito fracas com a maioria das outras variáveis.
*   **Correlações Negligenciáveis**:
    *   `event_id`, `latitude`, `longitude` não mostram correlações lineares significativas com as outras variáveis, como esperado.

#### Recomendações e Próximos Passos:
1.  **Transformação de Variáveis**: Considerar aplicar transformações (e.g., log, raiz quadrada) em `affected_population` e `estimated_economic_loss_usd` para lidar com a assimetria e reduzir o impacto de outliers em modelos, se necessário.
2.  **Engenharia de Features**: A coluna `date` deve ser convertida para `datetime` para extrair features temporais (ano, mês, dia da semana, estação) que podem ser relevantes.
3.  **Variáveis Categóricas**: As colunas `disaster_type`, `location` e `aid_provided` são categóricas. Será necessário aplicar técnicas de codificação (e.g., One-Hot Encoding, Label Encoding) antes da modelagem.
4.  **Modelagem**: Para a regressão (`severity_level`), `affected_population` e `estimated_economic_loss_usd` serão preditores chave. Para a classificação (`is_major_disaster`), as mesmas variáveis, juntamente com `infrastructure_damage_index`, serão importantes.

#Modelagem
A seguir, serão realizadas as seguinte tarefas:

- Pré-processamento de Dados

- Divisão dos Dados

- Modelagem de Regressão (severity_level)

- Modelagem de Classificação (is_major_disaster)

- Regressão Logística (Validação)

- Visualização dos principais Resultados da Modelagem de Dados

## Pré-Processamento de dados
"""

df['date'] = pd.to_datetime(df['date'])
df['event_year'] = df['date'].dt.year
df['event_month'] = df['date'].dt.month
df['event_dayofweek'] = df['date'].dt.dayofweek
df['event_weekofyear'] = df['date'].dt.isocalendar().week.astype(int)

print("Date column converted and temporal features extracted.")
print(df[['date', 'event_year', 'event_month', 'event_dayofweek', 'event_weekofyear']].head())

categorical_cols = ['disaster_type', 'location', 'aid_provided']
df_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=True, dtype=int)

# Drop the original 'date' column as temporal features have been extracted
df_processed = df_encoded.drop('date', axis=1)

print("Categorical columns One-Hot Encoded and original date column dropped.")
print(df_processed.head())

df_processed['affected_population_log'] = np.log1p(df_processed['affected_population'])
df_processed['estimated_economic_loss_usd_log'] = np.log1p(df_processed['estimated_economic_loss_usd'])

print("Logarithmic transformations applied to 'affected_population' and 'estimated_economic_loss_usd'.")
print(df_processed[['affected_population', 'affected_population_log', 'estimated_economic_loss_usd', 'estimated_economic_loss_usd_log']].head())

"""## Divisão dos Dados (Treino/Validação/Teste)

"""

from sklearn.model_selection import train_test_split

print("train_test_split imported successfully.")

X = df_processed.drop(['severity_level', 'is_major_disaster', 'affected_population', 'estimated_economic_loss_usd', 'event_id'], axis=1)
y_regression = df_processed['severity_level']
y_classification = df_processed['is_major_disaster']

# Split for regression
X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X, y_regression, test_size=0.2, random_state=42)
X_train_reg, X_val_reg, y_train_reg, y_val_reg = train_test_split(X_train_reg, y_train_reg, test_size=0.25, random_state=42) # 0.25 of 0.8 is 0.2

# Split for classification
X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(X, y_classification, test_size=0.2, random_state=42, stratify=y_classification)
X_train_clf, X_val_clf, y_train_clf, y_val_clf = train_test_split(X_train_clf, y_train_clf, test_size=0.25, random_state=42, stratify=y_train_clf)

print("Data split for Regression:")
print(f"X_train_reg shape: {X_train_reg.shape}, y_train_reg shape: {y_train_reg.shape}")
print(f"X_val_reg shape: {X_val_reg.shape}, y_val_reg shape: {y_val_reg.shape}")
print(f"X_test_reg shape: {X_test_reg.shape}, y_test_reg shape: {y_test_reg.shape}")

print("\nData split for Classification:")
print(f"X_train_clf shape: {X_train_clf.shape}, y_train_clf shape: {y_train_clf.shape}")
print(f"X_val_clf shape: {X_val_clf.shape}, y_val_clf shape: {y_val_clf.shape}")
print(f"X_test_clf shape: {X_test_clf.shape}, y_test_clf shape: {y_test_clf.shape}")

"""## Modelagem de Regressão (severity_level) - Statsmodels"""

import statsmodels.api as sm

print("statsmodels.api imported as sm.")

"""### Regressão Linear Simples (affected_population_log)

"""

X_train_reg_const_simple = sm.add_constant(X_train_reg['affected_population_log'])
simple_model = sm.OLS(y_train_reg, X_train_reg_const_simple).fit()
print("\nSimple Linear Regression Model Summary (affected_population_log vs severity_level):\n")
print(simple_model.summary())

"""### Regressão Linear Múltipla

"""

selected_features = ['affected_population_log', 'estimated_economic_loss_usd_log', 'infrastructure_damage_index']
X_train_reg_multi = X_train_reg[selected_features]
X_train_reg_const_multi = sm.add_constant(X_train_reg_multi)

multi_model = sm.OLS(y_train_reg, X_train_reg_const_multi).fit()
print("""
Multiple Linear Regression Model Summary (selected features vs severity_level):
""")
print(multi_model.summary())

"""## Modelagem de Regressão (severity_level) - Sklearn/PyCaret"""

baseline_mean = y_train_reg.mean()
print(f"Regression Baseline (Mean of y_train_reg): {baseline_mean:.2f}")

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np # Import numpy for sqrt function

print("Sklearn regression models and metrics imported successfully.")

"""### Regressão Linear Simples (affected_population_log)

"""

model_slr = LinearRegression()
model_slr.fit(X_train_reg[['affected_population_log']], y_train_reg)

y_pred_slr = model_slr.predict(X_val_reg[['affected_population_log']])

r2_slr = r2_score(y_val_reg, y_pred_slr)
mae_slr = mean_absolute_error(y_val_reg, y_pred_slr)
mse_slr = mean_squared_error(y_val_reg, y_pred_slr)
rmse_slr = np.sqrt(mse_slr)

print("\nSimple Linear Regression Performance (Validation Set):")
print(f"R-squared: {r2_slr:.4f}")
print(f"MAE: {mae_slr:.4f}")
print(f"MSE: {mse_slr:.4f}")
print(f"RMSE: {rmse_slr:.4f}")

"""### Regressão Linear Múltipla"""

selected_features_mlr = ['affected_population_log', 'estimated_economic_loss_usd_log', 'infrastructure_damage_index']
model_mlr = LinearRegression()
model_mlr.fit(X_train_reg[selected_features_mlr], y_train_reg)

y_pred_mlr = model_mlr.predict(X_val_reg[selected_features_mlr])

r2_mlr = r2_score(y_val_reg, y_pred_mlr)
mae_mlr = mean_absolute_error(y_val_reg, y_pred_mlr)
mse_mlr = mean_squared_error(y_val_reg, y_pred_mlr)
rmse_mlr = np.sqrt(mse_mlr)

print("\nMultiple Linear Regression Performance (Validation Set):")
print(f"R-squared: {r2_mlr:.4f}")
print(f"MAE: {mae_mlr:.4f}")
print(f"MSE: {mse_mlr:.4f}")
print(f"RMSE: {rmse_mlr:.4f}")

"""### Regressão Linear Polinomial

"""

from sklearn.preprocessing import PolynomialFeatures

# Define features for polynomial regression
poly_features = ['affected_population_log', 'estimated_economic_loss_usd_log', 'infrastructure_damage_index']

# Create PolynomialFeatures object
poly = PolynomialFeatures(degree=2, include_bias=False)
X_train_poly = poly.fit_transform(X_train_reg[poly_features])
X_val_poly = poly.transform(X_val_reg[poly_features])

# Train Polynomial Regression model
model_poly = LinearRegression()
model_poly.fit(X_train_poly, y_train_reg)

# Predict on validation set
y_pred_poly = model_poly.predict(X_val_poly)

# Calculate evaluation metrics for Polynomial Regression
r2_poly = r2_score(y_val_reg, y_pred_poly)
mae_poly = mean_absolute_error(y_val_reg, y_pred_poly)
mse_poly = mean_squared_error(y_val_reg, y_pred_poly)
rmse_poly = np.sqrt(mse_poly)

print("\nPolynomial Regression Performance (Validation Set):")
print(f"R-squared: {r2_poly:.4f}")
print(f"MAE: {mae_poly:.4f}")
print(f"MSE: {mse_poly:.4f}")
print(f"RMSE: {rmse_poly:.4f}")

"""### Resultados


"""

print("\n--- Regression Model Performance Summary (Validation Set) ---")
print(f"Regression Baseline (Mean of y_train_reg): {baseline_mean:.2f}")

print("\nSimple Linear Regression Performance:")
print(f"R-squared: {r2_slr:.4f}")
print(f"MAE: {mae_slr:.4f}")
print(f"MSE: {mse_slr:.4f}")
print(f"RMSE: {rmse_slr:.4f}")

print("\nMultiple Linear Regression Performance:")
print(f"R-squared: {r2_mlr:.4f}")
print(f"MAE: {mae_mlr:.4f}")
print(f"MSE: {mse_mlr:.4f}")
print(f"RMSE: {rmse_mlr:.4f}")

print("\nPolynomial Regression Performance:")
print(f"R-squared: {r2_poly:.4f}")
print(f"MAE: {mae_poly:.4f}")
print(f"MSE: {mse_poly:.4f}")
print(f"RMSE: {rmse_poly:.4f}")

"""## Modelagem de Classificação (is_major_disaster)


"""

majority_class_count = y_train_clf.value_counts().max()
total_samples = len(y_train_clf)
classification_baseline = majority_class_count / total_samples
print(f"Classification Baseline (Majority Class Proportion in y_train_clf): {classification_baseline:.4f}")

from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

print("Sklearn classification models and metrics imported successfully.")

"""###Naive Bayes


"""

model_gnb = GaussianNB()
model_gnb.fit(X_train_clf, y_train_clf)
y_pred_gnb = model_gnb.predict(X_val_clf)

accuracy_gnb = accuracy_score(y_val_clf, y_pred_gnb)
precision_gnb = precision_score(y_val_clf, y_pred_gnb)
recall_gnb = recall_score(y_val_clf, y_pred_gnb)
f1_gnb = f1_score(y_val_clf, y_pred_gnb)
cm_gnb = confusion_matrix(y_val_clf, y_pred_gnb)

print("\nNaive Bayes Classifier Performance (Validation Set):")
print(f"Accuracy: {accuracy_gnb:.4f}")
print(f"Precision: {precision_gnb:.4f}")
print(f"Recall: {recall_gnb:.4f}")
print(f"F1-Score: {f1_gnb:.4f}")
print("Confusion Matrix:\n", cm_gnb)

"""### Regressão Logística

"""

from sklearn.preprocessing import StandardScaler

# Scale the features for Logistic Regression
scaler = StandardScaler()
X_train_clf_scaled = scaler.fit_transform(X_train_clf)
X_val_clf_scaled = scaler.transform(X_val_clf)

model_lr = LogisticRegression(max_iter=1000, random_state=42)
model_lr.fit(X_train_clf_scaled, y_train_clf)
y_pred_lr = model_lr.predict(X_val_clf_scaled)

accuracy_lr = accuracy_score(y_val_clf, y_pred_lr)
precision_lr = precision_score(y_val_clf, y_pred_lr)
recall_lr = recall_score(y_val_clf, y_pred_lr)
f1_lr = f1_score(y_val_clf, y_pred_lr)
cm_lr = confusion_matrix(y_val_clf, y_pred_lr)

print("\nLogistic Regression Classifier Performance (Validation Set):")
print(f"Accuracy: {accuracy_lr:.4f}")
print(f"Precision: {precision_lr:.4f}")
print(f"Recall: {recall_lr:.4f}")
print(f"F1-Score: {f1_lr:.4f}")
print("Confusion Matrix:\n", cm_lr)

"""### Resultados


"""

print("\n--- Classification Model Performance Summary (Validation Set) ---")
print(f"Classification Baseline (Majority Class Proportion): {classification_baseline:.4f}")

print("\nNaive Bayes Classifier Performance:")
print(f"Accuracy: {accuracy_gnb:.4f}")
print(f"Precision: {precision_gnb:.4f}")
print(f"Recall: {recall_gnb:.4f}")
print(f"F1-Score: {f1_gnb:.4f}")

print("\nLogistic Regression Classifier Performance (after scaling):")
print(f"Accuracy: {accuracy_lr:.4f}")
print(f"Precision: {precision_lr:.4f}")
print(f"Recall: {recall_lr:.4f}")
print(f"F1-Score: {f1_lr:.4f}")

"""## Principais Resultados da Modelagem de Dados

###Pré-processamento de Dados:

- As variáveis temporais (year, month, day, dayofweek, weekofyear) foram extraídas com sucesso da coluna date.

- Variáveis categóricas (disaster_type, location, aid_provided) foram convertidas para formato numérico usando One-Hot Encoding.

- Transformações logarítmicas (np.log1p) foram aplicadas às variáveis affected_population e estimated_economic_loss_usd para lidar com assimetria e outliers, preparando os dados para modelagem.

###Divisão dos Dados:

- O conjunto pré-processado foi dividido em treino (60%), validação (20%) e teste (20%) para as tarefas de regressão (severity_level) e classificação (is_major_disaster).

- Para classificação, foi usada divisão estratificada para garantir representação equilibrada da variável-alvo.

### Modelagem de Regressão (severity_level):

- Baseline: A média da variável alvo y_train_reg foi 5,50.

- Regressão Linear Simples (statsmodels): affected_population_log explicou 29,0% da variância em severity_level (R² = 0,290), indicando relação positiva estatisticamente significativa.

- Regressão Linear Múltipla (statsmodels): Incluindo affected_population_log, estimated_economic_loss_usd_log e infrastructure_damage_index, o poder explicativo aumentou para 60,3% (R² = 0,603).

- affected_population_log e infrastructure_damage_index tiveram correlação positiva,

- estimated_economic_loss_usd_log mostrou correlação negativa.

- Desempenho dos Modelos (Sklearn – Validação):

- Regressão Linear Simples: R² = 0.2917, MAE = 2.1047, RMSE = 2.4328.

- Regressão Linear Múltipla: R² = 0.6019, MAE = 1.4809, RMSE = 1.8240.

- Regressão Polinomial (grau 2): Melhor desempenho entre os modelos de regressão:

- R² = 0.7900

- MAE = 1.0518

- RMSE = 1.3245
(Isso sugere relações não lineares.)

###Modelagem de Classificação (is_major_disaster):

Baseline: A classe majoritária representou 0.5999 do conjunto de treino.

- Naive Bayes (Validação):

- Acurácia: 0.8180

- F1-Score: 0.8115

- Recall alto (0.9794), mas precisão mais baixa (0.6927).

### Regressão Logística (Validação):
Após aplicar feature scaling (o que resolveu um aviso de convergência):

- Acurácia: 0.8940

- F1-Score: 0.8698
(Superou tanto o Naive Bayes quanto o baseline.)

### Percepções e Próximos Passos

Tarefa de Regressão:
A Regressão Polinomial (grau 2) é o modelo mais promissor para prever severity_level, com R² = 0.7900. Isso indica que considerar relações não lineares entre variáveis preditoras e a variável-alvo é vantajoso.

Tarefa de Classificação:
A Regressão Logística, especialmente após feature scaling, apresenta desempenho robusto (Acurácia: 0.8940, F1: 0.8698) na previsão de is_major_disaster, sendo uma candidata adequada para aprimoramentos futuros.

# Avaliação do desempenho

A seguir, serão realizadas as seguinte tarefas:
- Desempenho da Regressão Polinomial (Previsão do Nível de Severidade):
- Diagnóstico de Resíduos da Regressão Polinomial:
- Desempenho do Classificador Naive Bayes (Previsão de "Is Major Disaster"):
- Desempenho da Regressão Logística (Previsão de "Is Major Disaster"):
- Avaliação de resultados

## Desempenho da Regressão Polinomial (Previsão do Nível de Severidade)
"""

X_test_poly = poly.transform(X_test_reg[poly_features])
y_pred_poly_test = model_poly.predict(X_test_poly)

r2_poly_test = r2_score(y_test_reg, y_pred_poly_test)
mae_poly_test = mean_absolute_error(y_test_reg, y_pred_poly_test)
mse_poly_test = mean_squared_error(y_test_reg, y_pred_poly_test)
rmse_poly_test = np.sqrt(mse_poly_test)

print("\nPolynomial Regression Performance (Test Set):")
print(f"R-squared: {r2_poly_test:.4f}")
print(f"MAE: {mae_poly_test:.4f}")
print(f"MSE: {mse_poly_test:.4f}")
print(f"RMSE: {rmse_poly_test:.4f}")

"""## Diagnóstico de Resíduos da Regressão Polinomial

###normalidade
"""

import statsmodels.api as sm
import scipy.stats as stats

residuals_poly_test = y_test_reg - y_pred_poly_test

plt.figure(figsize=(8, 6))
sm.qqplot(residuals_poly_test, line='s', ax=plt.gca())
plt.title('Q-Q Plot of Polynomial Regression Residuals')
plt.xlabel('Theoretical Quantiles')
plt.ylabel('Standardized Residuals')
plt.grid(True)
plt.show()

print("Q-Q plot generated for Polynomial Regression residuals.")

"""###homocedasticidade

"""

plt.figure(figsize=(10, 6))
sns.scatterplot(x=y_pred_poly_test, y=residuals_poly_test, alpha=0.6)
plt.axhline(y=0, color='r', linestyle='--')
plt.title('Residuals vs. Predicted Values (Polynomial Regression)')
plt.xlabel('Predicted Severity Level')
plt.ylabel('Residuals')
plt.grid(True)
plt.show()

print("Residuals vs. Predicted Values plot generated for homoscedasticity check.")

"""###multicolinearidade (VIF)"""

from statsmodels.stats.outliers_influence import variance_inflation_factor

X_train_reg_multi_const = sm.add_constant(X_train_reg_multi)

vif_data = pd.DataFrame()
vif_data["feature"] = X_train_reg_multi_const.columns
vif_data["VIF"] = [variance_inflation_factor(X_train_reg_multi_const.values, i) for i in range(X_train_reg_multi_const.shape[1])]

print("\nVariance Inflation Factor (VIF) for Regression Model Features:\n")
print(vif_data)

"""##Desempenho do Modelo de Classificação

###Naive Bayes
"""

from sklearn.metrics import roc_auc_score

y_pred_gnb_test = model_gnb.predict(X_test_clf)
y_prob_gnb_test = model_gnb.predict_proba(X_test_clf)[:, 1]

accuracy_gnb_test = accuracy_score(y_test_clf, y_pred_gnb_test)
precision_gnb_test = precision_score(y_test_clf, y_pred_gnb_test)
recall_gnb_test = recall_score(y_test_clf, y_pred_gnb_test)
f1_gnb_test = f1_score(y_test_clf, y_pred_gnb_test)
auc_gnb_test = roc_auc_score(y_test_clf, y_prob_gnb_test)
cm_gnb_test = confusion_matrix(y_test_clf, y_pred_gnb_test)

print("\nNaive Bayes Classifier Performance (Test Set):")
print(f"Accuracy: {accuracy_gnb_test:.4f}")
print(f"Precision: {precision_gnb_test:.4f}")
print(f"Recall: {recall_gnb_test:.4f}")
print(f"F1-Score: {f1_gnb_test:.4f}")
print(f"AUC-ROC: {auc_gnb_test:.4f}")
print("Confusion Matrix:\n", cm_gnb_test)

"""### Regressão Logística"""

X_test_clf_scaled = scaler.transform(X_test_clf)
y_pred_lr_test = model_lr.predict(X_test_clf_scaled)
y_prob_lr_test = model_lr.predict_proba(X_test_clf_scaled)[:, 1]

accuracy_lr_test = accuracy_score(y_test_clf, y_pred_lr_test)
precision_lr_test = precision_score(y_test_clf, y_pred_lr_test)
recall_lr_test = recall_score(y_test_clf, y_pred_lr_test)
f1_lr_test = f1_score(y_test_clf, y_pred_lr_test)
auc_lr_test = roc_auc_score(y_test_clf, y_prob_lr_test)
cm_lr_test = confusion_matrix(y_test_clf, y_pred_lr_test)

print("\nLogistic Regression Classifier Performance (Test Set):")
print(f"Accuracy: {accuracy_lr_test:.4f}")
print(f"Precision: {precision_lr_test:.4f}")
print(f"Recall: {recall_lr_test:.4f}")
print(f"F1-Score: {f1_lr_test:.4f}")
print(f"AUC-ROC: {auc_lr_test:.4f}")
print("Confusion Matrix:\n", cm_lr_test)

"""## Principais Resultados da Análise de Dados

Desempenho da Regressão Polinomial (Previsão do Nível de Severidade):
- O modelo apresentou um desempenho sólido no conjunto de teste, explicando aproximadamente 79,03% da variância (R-quadrado: 0.7903). O erro médio de predição foi baixo, com MAE de 1.0375 e RMSE de 1.3107.

Diagnóstico de Resíduos da Regressão Polinomial:

- O gráfico Q-Q indicou uma leve divergência da normalidade nos resíduos, especialmente nas extremidades.

- O gráfico de resíduos versus valores previstos mostrou homoscedasticidade razoável, sem padrões claros ou formato de funil.

A análise de VIF revelou multicolinearidade significativa entre affected_population_log (VIF: 30.3065) e estimated_economic_loss_usd_log (VIF: 29.1957), enquanto infrastructure_damage_index apresentou VIF baixo (1.2208).

Desempenho do Classificador Naive Bayes (Previsão de "Is Major Disaster"):
- No conjunto de teste, o modelo Naive Bayes obteve Acurácia de 0.8067, Precisão de 0.6800, Recall de 0.9762, F1-Score de 0.8016 e AUC-ROC de 0.9313.
A matriz de confusão mostrou 38 falsos negativos, mas 735 falsos positivos, indicando alto recall porém precisão mais baixa.

Desempenho da Regressão Logística (Previsão de "Is Major Disaster"):
- O modelo de Regressão Logística superou o Naive Bayes, alcançando Acurácia de 0.8898, Precisão de 0.8540, Recall de 0.8738, F1-Score de 0.8638 e AUC-ROC de 0.9590.
Sua matriz de confusão apresentou desempenho mais equilibrado, com 202 falsos negativos e 239 falsos positivos.

Comparação dos Modelos:
- A Regressão Logística demonstrou desempenho superior na classificação de is_major_disaster em todas as métricas principais (Acurácia, Precisão, Recall, F1-Score, AUC-ROC) em comparação ao Naive Bayes, oferecendo melhor equilíbrio entre identificar corretamente desastres maiores e minimizar falsos alarmes.

Percepções ou Próximos Passos

- Abordando a Multicolinearidade na Regressão:
Embora a multicolinearidade não afete a acurácia preditiva, caso a interpretabilidade dos coeficientes se torne uma prioridade no modelo de Regressão Polinomial, pode ser útil investigar técnicas de seleção de variáveis ou abordagens alternativas de modelagem.

- Otimização Adicional dos Modelos:
Explorar modelos de regressão mais avançados (por exemplo, modelos baseados em árvores) para severity_level e realizar tuning de hiperparâmetros tanto nos modelos de regressão quanto nos de classificação pode levar a desempenhos ainda melhores.

#Otimização
A seguir serão realizadas as seguinte tarefas:
- Alteração de parametros (Regressão - Scikit-learn Grid/Random Search)
- Alteração de parametros (Classificação - Scikit-learn Grid/Random Search)
- Sumário Final de Otimizações

##Regressão
"""

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
import numpy as np
import pandas as pd

# --- Start of re-defining df_processed and data splits for independent execution ---

# 1. Load the dataset (from cell m7A6laXQWDCG)
df = pd.read_csv('synthetic_disaster_events_2025.csv')

# 2. Pre-processing step 1: Convert 'date' and extract temporal features (from cell 8c0a46ae)
df['date'] = pd.to_datetime(df['date'])
df['event_year'] = df['date'].dt.year
df['event_month'] = df['date'].dt.month
df['event_dayofweek'] = df['date'].dt.dayofweek
df['event_weekofyear'] = df['date'].dt.isocalendar().week.astype(int)

# 3. Pre-processing step 2: One-Hot Encode categorical columns (from cell f6045014)
categorical_cols = ['disaster_type', 'location', 'aid_provided']
df_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=True, dtype=int)
df_processed = df_encoded.drop('date', axis=1)

# 4. Pre-processing step 3: Apply logarithmic transformations (from cell 20c83a74)
df_processed['affected_population_log'] = np.log1p(df_processed['affected_population'])
df_processed['estimated_economic_loss_usd_log'] = np.log1p(df_processed['estimated_economic_loss_usd'])

# Re-execute data splitting (from cell e86888df)
X = df_processed.drop(['severity_level', 'is_major_disaster', 'affected_population', 'estimated_economic_loss_usd', 'event_id'], axis=1)
y_regression = df_processed['severity_level']

X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X, y_regression, test_size=0.2, random_state=42)
X_train_reg, X_val_reg, y_train_reg, y_val_reg = train_test_split(X_train_reg, y_train_reg, test_size=0.25, random_state=42) # 0.25 of 0.8 is 0.2

# Re-execute initial polynomial regression (from cell be493d34) to define r2_poly, etc.
poly_features_initial = ['affected_population_log', 'estimated_economic_loss_usd_log', 'infrastructure_damage_index']
poly_initial = PolynomialFeatures(degree=2, include_bias=False)
X_train_poly_initial = poly_initial.fit_transform(X_train_reg[poly_features_initial])
X_val_poly_initial = poly_initial.transform(X_val_reg[poly_features_initial])

model_poly_initial = LinearRegression()
model_poly_initial.fit(X_train_poly_initial, y_train_reg)
y_pred_poly_initial = model_poly_initial.predict(X_val_poly_initial)

r2_poly = r2_score(y_val_reg, y_pred_poly_initial)
mae_poly = mean_absolute_error(y_val_reg, y_pred_poly_initial)
mse_poly = mean_squared_error(y_val_reg, y_pred_poly_initial)
rmse_poly = np.sqrt(mse_poly)

# --- End of re-defining df_processed and data splits ---

# 1. Define a list of feature names
poly_features = ['affected_population_log', 'estimated_economic_loss_usd_log', 'infrastructure_damage_index']

# 2. Create a scikit-learn Pipeline
pipeline = Pipeline([
    ('poly', PolynomialFeatures(include_bias=False)),
    ('regressor', Ridge(random_state=42))
])

# 3. Define a dictionary param_grid for GridSearchCV
param_grid = {
    'poly__degree': [1, 2, 3],
    'regressor__alpha': [0.1, 1.0, 10.0]
}

# 4. Initialize GridSearchCV
grid_search_reg = GridSearchCV(
    estimator=pipeline,
    param_grid=param_grid,
    cv=5,
    scoring='neg_mean_squared_error',
    n_jobs=-1,
    verbose=1
)

# 5. Fit grid_search_reg to the training data
print("\nStarting GridSearchCV fit...")
grid_search_reg.fit(X_train_reg[poly_features], y_train_reg)
print("GridSearchCV fit complete.")

# 6. Retrieve the best estimator
best_poly_reg_model = grid_search_reg.best_estimator_

# 7. Retrieve the best parameters
best_poly_reg_params = grid_search_reg.best_params_

# 8. Print the best_poly_reg_params
print("\nBest Hyperparameters found by GridSearchCV for Polynomial Regression:")
print(best_poly_reg_params)

# 9. Predict on the validation set
y_pred_tuned_reg = best_poly_reg_model.predict(X_val_reg[poly_features])

# 10. Calculate evaluation metrics for the tuned model
r2_tuned_reg = r2_score(y_val_reg, y_pred_tuned_reg)
mae_tuned_reg = mean_absolute_error(y_val_reg, y_pred_tuned_reg)
mse_tuned_reg = mean_squared_error(y_val_reg, y_pred_tuned_reg)
rmse_tuned_reg = np.sqrt(mse_tuned_reg)

# 11. Print the calculated R-squared, MAE, MSE, and RMSE for the tuned model
print("\n--- Tuned Polynomial Regression Performance (Validation Set) ---")
print(f"R-squared (Tuned): {r2_tuned_reg:.4f}")
print(f"MAE (Tuned): {mae_tuned_reg:.4f}")
print(f"MSE (Tuned): {mse_tuned_reg:.4f}")
print(f"RMSE (Tuned): {rmse_tuned_reg:.4f}")

# 12. Print the previously obtained R-squared, MAE, MSE, and RMSE for comparison
print("\n--- Previous Polynomial Regression Performance (Validation Set) ---")
print(f"R-squared (Previous): {r2_poly:.4f}")
print(f"MAE (Previous): {mae_poly:.4f}")
print(f"MSE (Previous): {mse_poly:.4f}")
print(f"RMSE (Previous): {rmse_poly:.4f}")

# 13. Store all the results in a dictionary
regression_tuning_results = {
    'tuned_poly_reg': {
        'model_name': 'Tuned Polynomial Regression (Ridge)',
        'best_parameters': best_poly_reg_params,
        'r2': r2_tuned_reg,
        'mae': mae_tuned_reg,
        'mse': mse_tuned_reg,
        'rmse': rmse_tuned_reg
    },
    'previous_poly_reg': {
        'model_name': 'Polynomial Regression (Linear)',
        'r2': r2_poly,
        'mae': mae_poly,
        'mse': mse_poly,
        'rmse': rmse_poly
    }
}

print("\nRegression tuning results stored in 'regression_tuning_results' dictionary.")

"""##Classificação"""

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix
import numpy as np
import pandas as pd

# --- Start of re-defining df_processed and data splits for independent execution ---

# 1. Load the dataset (from cell m7A6laXQWDCG)
df = pd.read_csv('synthetic_disaster_events_2025.csv')

# 2. Pre-processing step 1: Convert 'date' and extract temporal features (from cell 8c0a46ae)
df['date'] = pd.to_datetime(df['date'])
df['event_year'] = df['date'].dt.year
df['event_month'] = df['date'].dt.month
df['event_dayofweek'] = df['date'].dt.dayofweek
df['event_weekofyear'] = df['date'].dt.isocalendar().week.astype(int)

# 3. Pre-processing step 2: One-Hot Encode categorical columns (from cell f6045014)
categorical_cols = ['disaster_type', 'location', 'aid_provided']
df_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=True, dtype=int)
df_processed = df_encoded.drop('date', axis=1)

# 4. Pre-processing step 3: Apply logarithmic transformations (from cell 20c83a74)
df_processed['affected_population_log'] = np.log1p(df_processed['affected_population'])
df_processed['estimated_economic_loss_usd_log'] = np.log1p(df_processed['estimated_economic_loss_usd'])

# Re-execute data splitting for classification (from cell e86888df)
X = df_processed.drop(['severity_level', 'is_major_disaster', 'affected_population', 'estimated_economic_loss_usd', 'event_id'], axis=1)
y_classification = df_processed['is_major_disaster']

X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(X, y_classification, test_size=0.2, random_state=42, stratify=y_classification)
X_train_clf, X_val_clf, y_train_clf, y_val_clf = train_test_split(X_train_clf, y_train_clf, test_size=0.25, random_state=42, stratify=y_train_clf)

# Re-execute previous Logistic Regression to get comparison metrics (from cell 8d294cdc)
scaler = StandardScaler()
X_train_clf_scaled = scaler.fit_transform(X_train_clf)
X_val_clf_scaled = scaler.transform(X_val_clf)

model_lr = LogisticRegression(max_iter=1000, random_state=42)
model_lr.fit(X_train_clf_scaled, y_train_clf)
y_pred_lr = model_lr.predict(X_val_clf_scaled)
y_prob_lr = model_lr.predict_proba(X_val_clf_scaled)[:, 1]

accuracy_lr = accuracy_score(y_val_clf, y_pred_lr)
precision_lr = precision_score(y_val_clf, y_pred_lr)
recall_lr = recall_score(y_val_clf, y_pred_lr)
f1_lr = f1_score(y_val_clf, y_pred_lr)
auc_lr = roc_auc_score(y_val_clf, y_prob_lr)

# --- End of re-defining df_processed and data splits and previous LR ---


# 1. Define the list of feature names to be used for classification
clf_features = X_train_clf.columns.tolist()

# 2. Create a scikit-learn Pipeline
pipeline_clf = Pipeline([
    ('scaler', StandardScaler()),
    ('logreg', LogisticRegression(random_state=42, max_iter=1000))
])

# 3. Define a dictionary param_grid for GridSearchCV
param_grid_clf = {
    'logreg__C': [0.01, 0.1, 1, 10, 100],  # Inverse of regularization strength
    'logreg__penalty': ['l2'], # L2 regularization is common
    'logreg__solver': ['liblinear', 'lbfgs'] # Solvers compatible with 'l2'
}

# For 'l1' penalty, 'liblinear' and 'saga' are compatible solvers.
# If 'l1' were to be included:
# param_grid_clf = [
#     {'logreg__C': [0.01, 0.1, 1, 10, 100], 'logreg__penalty': ['l1'], 'logreg__solver': ['liblinear']},
#     {'logreg__C': [0.01, 0.1, 1, 10, 100], 'logreg__penalty': ['l2'], 'logreg__solver': ['liblinear', 'lbfgs']}
# ]

# 4. Initialize GridSearchCV
grid_search_clf = GridSearchCV(
    estimator=pipeline_clf,
    param_grid=param_grid_clf,
    cv=5,
    scoring='f1',
    n_jobs=-1,
    verbose=1
)

# 5. Fit the grid_search_clf object to the training data
print("\nStarting GridSearchCV fit for classification...")
grid_search_clf.fit(X_train_clf[clf_features], y_train_clf)
print("GridSearchCV fit for classification complete.")

# 6. Retrieve the best estimator
best_lr_model = grid_search_clf.best_estimator_

# 7. Retrieve the best parameters
best_lr_params = grid_search_clf.best_params_

# 8. Print the best_lr_params
print("\nBest Hyperparameters found by GridSearchCV for Logistic Regression:")
print(best_lr_params)

# 9. Use the best_lr_model to make predictions on the validation set
y_pred_tuned_clf = best_lr_model.predict(X_val_clf[clf_features])
y_prob_tuned_clf = best_lr_model.predict_proba(X_val_clf[clf_features])[:, 1]

# Calculate evaluation metrics for the tuned model
accuracy_tuned_clf = accuracy_score(y_val_clf, y_pred_tuned_clf)
precision_tuned_clf = precision_score(y_val_clf, y_pred_tuned_clf)
recall_tuned_clf = recall_score(y_val_clf, y_pred_tuned_clf)
f1_tuned_clf = f1_score(y_val_clf, y_pred_tuned_clf)
auc_tuned_clf = roc_auc_score(y_val_clf, y_prob_tuned_clf)

# 10. Print the calculated metrics for the tuned model
print("\n--- Tuned Logistic Regression Performance (Validation Set) ---")
print(f"Accuracy (Tuned): {accuracy_tuned_clf:.4f}")
print(f"Precision (Tuned): {precision_tuned_clf:.4f}")
print(f"Recall (Tuned): {recall_tuned_clf:.4f}")
print(f"F1-Score (Tuned): {f1_tuned_clf:.4f}")
print(f"AUC-ROC (Tuned): {auc_tuned_clf:.4f}")

# 11. Print the previously obtained performance metrics for comparison
print("\n--- Previous Logistic Regression Performance (Validation Set) ---")
print(f"Accuracy (Previous): {accuracy_lr:.4f}")
print(f"Precision (Previous): {precision_lr:.4f}")
print(f"Recall (Previous): {recall_lr:.4f}")
print(f"F1-Score (Previous): {f1_lr:.4f}")
print(f"AUC-ROC (Previous): {auc_lr:.4f}")

# 12. Store all the results in a dictionary
classification_tuning_results = {
    'tuned_logreg': {
        'model_name': 'Tuned Logistic Regression',
        'best_parameters': best_lr_params,
        'accuracy': accuracy_tuned_clf,
        'precision': precision_tuned_clf,
        'recall': recall_tuned_clf,
        'f1_score': f1_tuned_clf,
        'auc_roc': auc_tuned_clf
    },
    'previous_logreg': {
        'model_name': 'Logistic Regression (Untuned)',
        'accuracy': accuracy_lr,
        'precision': precision_lr,
        'recall': recall_lr,
        'f1_score': f1_lr,
        'auc_roc': auc_lr
    }
}

print("\nClassification tuning results stored in 'classification_tuning_results' dictionary.")

"""## Sumário Final da Otimização de Hiperparâmetros e Comparação de Modelos

### Regressão (Previsão de `severity_level`)

**Modelo Base (Pré-Tuning): Regressão Polinomial (Grau 2, LinearRegression)**
-   **R-squared (Validação):** 0.7900
-   **MAE (Validação):** 1.0518
-   **RMSE (Validação):** 1.3245

**Melhor Modelo Tuned: Regressão Polinomial (com Ridge)**
-   **Melhores Parâmetros:** `{'poly__degree': 3, 'regressor__alpha': 0.1}`
-   **R-squared (Validação):** 0.8183
-   **MAE (Validação):** 0.9775
-   **RMSE (Validação):** 1.2323

**Análise e Melhorias:**
A otimização de hiperparâmetros para o modelo de regressão polinomial, incorporando regularização Ridge e explorando um grau polinomial de até 3, resultou em melhorias significativas. O R-squared aumentou de 0.7900 para 0.8183, indicando que o modelo tunado explica uma porção maior da variância na variável `severity_level`. As métricas de erro (MAE e RMSE) também diminuíram, sugerindo que o modelo tunado faz previsões mais precisas. A escolha de `degree=3` sugere que relações cúbicas entre as features e a variável alvo são benéficas, e um pequeno `alpha` para a regularização Ridge (`0.1`) ajudou a manter a complexidade do modelo sem penalizar excessivamente os coeficientes.

### Classificação (Previsão de `is_major_disaster`)

**Modelo Base (Pré-Tuning): Regressão Logística (com StandardScaler)**
-   **Acurácia (Validação):** 0.8940
-   **Precisão (Validação):** 0.8551
-   **Recall (Validação):** 0.8850
-   **F1-Score (Validação):** 0.8698
-   **AUC-ROC (Validação):** 0.9632

**Melhor Modelo Tuned: Regressão Logística (com StandardScaler)**
-   **Melhores Parâmetros:** `{'logreg__C': 1, 'logreg__penalty': 'l2', 'logreg__solver': 'lbfgs'}`
-   **Acurácia (Validação):** 0.8940
-   **Precisão (Validação):** 0.8551
-   **Recall (Validação):** 0.8850
-   **F1-Score (Validação):** 0.8698
-   **AUC-ROC (Validação):** 0.9632

**Análise e Melhorias:**
Para o modelo de Regressão Logística, a otimização de hiperparâmetros não resultou em mudanças notáveis nas métricas de desempenho no conjunto de validação. Isso ocorre porque o modelo original (com `max_iter=1000` e `random_state=42`) já apresentava um desempenho muito próximo ao ideal dentro do espaço de parâmetros explorado. Os melhores parâmetros (`C=1`, `penalty='l2'`, `solver='lbfgs'`) correspondem a valores comuns e sensatos para a regularização L2 e o solver `lbfgs` é eficiente para este tipo de problema. Isso sugere que o modelo já estava bem ajustado, ou que o espaço de busca dos hiperparâmetros não continha valores que pudessem melhorar significativamente o desempenho além do que já foi alcançado.

### Conclusões e Trade-offs

*   **Regressão (severity_level):** A otimização de hiperparâmetros foi altamente benéfica para o modelo de regressão, melhorando sua capacidade preditiva e reduzindo erros. A transição para um grau polinomial maior (`degree=3`) e a inclusão de regularização Ridge foram cruciais. O trade-off aqui é o aumento na complexidade do modelo, mas a melhoria no desempenho justifica essa complexidade adicionada.

*   **Classificação (is_major_disaster):** O modelo de Regressão Logística já estava robusto antes da otimização, e as métricas de desempenho permaneceram as mesmas após o tuning. Isso pode indicar que o modelo já estava operando perto de seu potencial máximo para os dados e features disponíveis, ou que o range de parâmetros explorados não era amplo o suficiente para encontrar melhorias substanciais. Não houve um trade-off significativo, pois o desempenho se manteve estável.

Em resumo, a otimização de hiperparâmetros confirmou o bom desempenho dos modelos selecionados e, no caso da regressão, proporcionou uma melhoria notável. Para a classificação, o modelo de Regressão Logística mantém sua posição como o melhor, validando sua robustez.
"""